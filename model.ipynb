{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIVjtVUkExBZ"
   },
   "source": [
    "**Kaggle Challenge: Google-QUEST-Q-A-Labeling**\n",
    "\n",
    "This challenge is mainly regression based, where each example data consists of a few question and answer features respectively, and 30 output variables, whose values have to be estimated. The following notebook consists of the central BERT-based model which has been used for this challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6SG7owadpvk3",
    "outputId": "e0a101db-5642-4bf5-a0d2-aff5e154713b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount('/content/gdrive', force_remount = True)\n",
    "dataset_path = 'gdrive/My Drive/Projects/quest/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h9o7O-MHp5yB",
    "outputId": "cc83dbb6-fc4a-4552-89f6-609d5fa44bdf"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.1.0-rc2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "FPnOIig1q4qZ",
    "outputId": "276121cf-aebf-4fe1-e6a3-afc35f90807c"
   },
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "nR8erWrQrv3U",
    "outputId": "20090748-b202-4189-b186-5524682a6d29"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SWP4M8b_ryZI",
    "outputId": "3de5b2ff-2a22-416d-8b2c-9353f5c18444"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwW7OAKZEb_i"
   },
   "source": [
    "This cell specifies the **BERT tokenizer** to be used, and reads the data. Here, the uncased bert version is used with 12 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "2zA64IbXr2au",
    "outputId": "525df0ce-d37b-44fa-902a-a42d77c3ed5d"
   },
   "outputs": [],
   "source": [
    "PATH = dataset_path\n",
    "\n",
    "# BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n",
    "# tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n",
    "\n",
    "BERT_PATH = dataset_path + 'bert-base-uncased-huggingface-transformer/'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt',do_lower_case = True)\n",
    "\n",
    "#tokenizer.add_tokens(['[Q-TITLE]'])\n",
    "#l = len(tokenizer)\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFdePahqIabE"
   },
   "source": [
    "Retrieving the input features and output categories of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "aAMFljm6r5ux",
    "outputId": "d488a7b7-1351-454b-f472-42d7a47dcea7"
   },
   "outputs": [],
   "source": [
    "output_categories_qn = list(df_train.columns[11:32])\n",
    "output_categories_ans = list(df_train.columns[32:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])\n",
    "print('\\noutput categories:\\n\\t', len(output_categories_qn))\n",
    "print('\\ninput categories:\\n\\t', len(output_categories_ans))\n",
    "output_categories = output_categories_qn+output_categories_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_yWylYiIo_V"
   },
   "source": [
    "**Processing of Input Data**\n",
    "\n",
    "Each input example consists of the question title, question body, and the answer body. These input examples are then passed on to the BERT tokenizer, in two ways (one consists of the question title and body, the other consists of the question title and answer body), which then separates the data into two sets of ids, masks and segments, one for the question, the other for the answer.\n",
    "\n",
    "BERT accepts input vectors of length 512 only. To incorporate most information into the input vectors, the first 200 components and the last 312 components are taken of the ids and segments that are obtained as outputs of the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zY3pMaC5r8iW"
   },
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            )\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        if len(input_ids) > length:\n",
    "          input_ids = input_ids[:200] + input_ids[-312:]\n",
    "          input_segments = input_segments[:200] + input_segments[-312:]\n",
    "\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title+\" \"+question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        title + \" \" + answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BOneHEnRVgMe",
    "outputId": "44d67ee9-5d5a-47d7-f1d3-edfb85796acd"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jT1r0XM1KPgB"
   },
   "source": [
    "**BERT - LSTM model**\n",
    "\n",
    "This model is a concatenation of two branch models, one of the question and one for the answer. The basic construction of the two branches is the same. Both take as input their respective id, mask and segment, pass them onto the pretrained BERT model. Thereafter, the last four hidden layers of the BERT model are concatenated and passed on to a bi-LSTM of 512 cells. This layer is passed on to a pooling layer which is the final layer of the branch. After that, the two branches are concatenated, to which a drop out layer is added. The next layer is the output layer consisting of 30 cells for the corresponding output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdoMk-Brr_bY"
   },
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def create_model_qn():\n",
    "    q_id_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    #bert_model.resize_token_embeddings(30523)\n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    #outputs = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[2]\n",
    "    \n",
    "    \n",
    "    #l_1, l_2, l_3, l_4 = outputs[-1], outputs[-2], outputs[-3], outputs[-4]\n",
    "\n",
    "    q_embedding_1 = bert_model(q_id_1, attention_mask=q_mask_1, token_type_ids=q_atn_1)[0]\n",
    "    \n",
    "    #q_embedding = tf.keras.layers.concatenate([l_1, l_2, l_3, l_4])\n",
    "    \n",
    "    q_1 = tf.keras.layers.GlobalAveragePooling1D()(q_embedding_1)\n",
    "        \n",
    "    x_1 = tf.keras.layers.Dropout(0.2)(q_1)\n",
    "    \n",
    "    x_1 = tf.keras.layers.Dense(21, activation='sigmoid')(x_1)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id_1, q_mask_1, q_atn_1], outputs=x_1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_ans():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    \n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    \n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    #bert_model.resize_token_embeddings(30523)\n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    outputs = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[2]\n",
    "    \n",
    "    l_1, l_2, l_3, l_4 = outputs[-1], outputs[-2], outputs[-3], outputs[-4]\n",
    "    \n",
    "    q_embedding = tf.keras.layers.concatenate([l_1, l_2, l_3, l_4])\n",
    "    q_embedding = Bidirectional(LSTM(512, return_sequences=True))(q_embedding)\n",
    "    #q_embedding = Bidirectional(LSTM(128, return_sequences=True))(q_embedding)\n",
    "    #q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    \n",
    "    outputs_ans = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[2]\n",
    "    \n",
    "    a_1, a_2, a_3, a_4 = outputs_ans[-1], outputs_ans[-2], outputs_ans[-3], outputs_ans[-4]\n",
    "    \n",
    "    a_embedding = tf.keras.layers.concatenate([a_1, a_2, a_3, a_4])\n",
    "    a_embedding = Bidirectional(LSTM(512, return_sequences=True))(a_embedding)\n",
    "    #a_embedding = Bidirectional(LSTM(128, return_sequences=True))(a_embedding)\n",
    "    #q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    #a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,a_id, a_mask, a_atn], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "LOkMNdm4sBXR",
    "outputId": "c9c7a6b8-f483-4e4f-dc1d-c40c1b9e0b57"
   },
   "outputs": [],
   "source": [
    "#outputs_qn = compute_output_arrays(df_train, output_categories_qn)\n",
    "#outputs_ans = compute_output_arrays(df_train, output_categories_ans)\n",
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nts8MCUxNCAl"
   },
   "source": [
    "The training is performed at this stage, with 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "p7VCq1DnsDZS",
    "outputId": "32e8eafe-d00e-4cfb-ad2f-b35daf9a6051"
   },
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "K.clear_session()\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    # will actually only do 2 folds (out of 5) to manage < 2h\n",
    "    if fold in range(10):\n",
    "\n",
    "        #train_inputs_qn = [inputs[i][train_idx] for i in range(3)]\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "        train_outputs = outputs[train_idx]\n",
    "        #train_outputs_qn = outputs_qn[train_idx]\n",
    "        #train_outputs_ans = outputs_ans[train_idx]\n",
    "        \n",
    "        #valid_inputs_qn = [inputs[i][valid_idx] for i in range(3)]\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "        #valid_outputs_qn = outputs_qn[valid_idx]\n",
    "        #valid_outputs_ans = outputs_ans[valid_idx]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #model = create_model_qn()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "        #model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        #model.fit(train_inputs_qn, train_outputs_qn, epochs=1, batch_size=6)\n",
    "        #model.save_weights(dataset_path + 'bert-ques'+str(fold)+'.h5')'''\n",
    "        K.clear_session()\n",
    "        model1 = create_model_ans()\n",
    "        model1.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        model1.fit(train_inputs, train_outputs, epochs=3, batch_size=6)\n",
    "        model1.save_weights(dataset_path + 'bert-ans'+str(fold)+'.h5')\n",
    "        \n",
    "        valid_preds.append(model1.predict(valid_inputs))\n",
    "        test_preds.append(model1.predict(test_inputs))\n",
    "        #valid_outputs = np.column_stack((valid_outputs_qn,valid_outputs_ans))\n",
    "        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "        print('validation score = ', rho_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UT5TbdMNikb"
   },
   "source": [
    "There are a number of variants of the model mentioned above. \n",
    "\n",
    "1.   Use bi-GRU of the same number of cells instead of bi_LSTM\n",
    "2.   Adjust the number of LSTM/GRU cells\n",
    "3.   Instead of concatenating the last 4 hidden layers of the BERT model and passing it to bi-LSTM or bi-GRU, use the original output of the BERT model\n",
    "4.   Instead of creating two branches for questions and answers in the same model, create two separate models. One model will take into account the question based features and predict only the question based output variables, the other model will take in the question and answer based features, and predict the answer based output variables. The structure of the model can be any of the above 3 architectures. However, this has not been tried, as it usually consumes a lot of resource.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJNPIHdbtB6a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
